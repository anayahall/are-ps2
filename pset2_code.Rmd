---
title: 'Problem Set #2'
author: "Anaya Hall & Christian Miller"
output: pdf_document
fontsize: 11pt
geometry: margin=.75in 
---

# Part 1: Theory
(for practice only)

# Part 2: Applied - Returns to Scale in Electricity Supply

```{r Setup, include=F}
rm(list = ls())
# Setup
knitr::opts_chunk$set(echo = T, cache = T)
# Options
options(stringsAsFactors = F)
# Packages
library(pacman)
p_load(knitr, kableExtra, WDI, dplyr, readr, magrittr, ggplot2, readxl, smartypants)

# GGplot2 theme----
theme_are <- theme(
  legend.position = "bottom",
  panel.background = element_rect(fill = NA),
  panel.border = element_rect(fill = NA, color = "grey75"),
  axis.ticks = element_line(color = "grey85"),
  panel.grid.major = element_line(color = "grey95", size = 0.2),
  panel.grid.minor = element_line(color = "grey95", size = 0.2),
  legend.key = element_blank()
)

```


First, load our OLS function created in Problem Set #1. We're including a built in t-test this time around.

```{r OLS function}
ols <- function(data, y_data, X_data, intercept = T, H0 = 0, two_tail = T, alpha = 0.05) {
  # Function setup ----
    # Require the 'dplyr' package
    require(dplyr)
    # Function to convert tibble, data.frame, or tbl_df to matrix
    to_matrix <- function(the_df, vars) {
      # Create a matrix from variables in var
      new_mat <- the_df %>%
        #Select the columns given in 'vars'
        select_(.dots = vars) %>%
        # Convert to matrix
        as.matrix()
      # Return 'new_mat'
      return(new_mat)
    }
  
  # Create dependent and independent variable matrices ----
    # y matrix
    y <- to_matrix (the_df = data, vars = y_data)
    # X matrix
    X <- to_matrix (the_df = data, vars = X_data)
      # If 'intercept' is TRUE, then add a column of ones
      if (intercept == T) {
      X <- cbind(1,X)
      colnames(X) <- c("intercept", X_data)
      }
 
  # Calculate b, y_hat, and residuals ----
    b <- solve(t(X) %*% X) %*% t(X) %*% y
    y_hat <- X %*% b
    e <- y - y_hat
  
  # Useful -----
    n <- nrow(X) # number of observations
    k <- ncol(X) # number of independent variables
    dof <- n - k # degrees of freedom
    i <- rep(1,n) # column of ones for demeaning matrix
    A <- diag(i) - (1 / n) * i %*% t(i) # demeaning matrix
    y_star <- A %*% y # for SST
    X_star <- A %*% X # for SSM
    SST <- drop(t(y_star) %*% y_star)
    SSM <- drop(t(b) %*% t(X_star) %*% X_star %*% b)
    SSR <- drop(t(e) %*% e)
  
  # Measures of fit and estimated variance ----
    R2uc <- drop((t(y_hat) %*% y_hat)/(t(y) %*% y)) # Uncentered R^2
    R2 <- 1 - SSR/SST # Uncentered R^2
    R2adj <- 1 - (n-1)/dof * (1 - R2) # Adjusted R^2
    AIC <- log(SSR/n) + 2*k/n # AIC
    SIC <- log(SSR/n) + k/n*log(n) # SIC
    s2 <- SSR/dof # s^2
  
  # Measures of fit table ----
    mof_table_df <- data.frame(R2uc, R2, R2adj, SIC, AIC, SSR, s2)
    mof_table_col_names <- c("$R^2_\\text{uc}$", "$R^2$",
                             "$R^2_\\text{adj}$",
                             "SIC", "AIC", "SSR", "$s^2$")
    mof_table <-  mof_table_df %>% knitr::kable(
      row.names = F,
      col.names = mof_table_col_names,
      format.args = list(scientific = F, digits = 4),
      booktabs = T,
      escape = F
    )
  
  # t-test----
    # Standard error
    se <- as.vector(sqrt(s2 * diag(solve(t(X) %*% X))))
    # Vector of _t_ statistics
    t_stats <- (b - H0) / se
    # Calculate the p-values
    if (two_tail == T) {
    p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F) * 2
    } else {
      p_values <- pt(q = abs(t_stats), df = dof, lower.tail = F)
    }
    # Do we (fail to) reject?
    reject <- ifelse(p_values < alpha, reject <- "Reject", reject <- "Fail to Reject")
    
    # Nice table (data.frame) of results
    ttest_df <- data.frame(
      # The rows have the coef. names
      effect = rownames(b),
      # Estimated coefficients
      coef = as.vector(b) %>% round(3),
      # Standard errors
      std_error = as.vector(se) %>% round(3),
      # t statistics
      t_stat = as.vector(t_stats) %>% round(3),
      # p-values
      p_value = as.vector(p_values) %>% round(4),
      # reject null?
      significance = as.character(reject)
      )
  
    ttest_table <-  ttest_df %>% knitr::kable(
      booktabs = T,
      format.args = list(scientific = F),
      escape = F
    )

  # Data frame for exporting for y, y_hat, X, and e vectors ----
    export_df <- data.frame(y, y_hat, e, X) %>% tbl_df()
    colnames(export_df) <- c("y","y_hat","e",colnames(X))
  
  # Return ----
    return(list(n=n, dof=dof, b=b, vars=export_df, R2uc=R2uc,R2=R2,
                R2adj=R2adj, AIC=AIC, SIC=SIC, s2=s2, SST=SST, SSR=SSR,
                mof_table=mof_table, ttest=ttest_table))
}
```

We'll also need a function to do an F-test for this Problem Set.

```{r F-test function}

# Joint test function (from Ed's notes). Could also write a more complex functions that takes R and r. 
F_test <- function(data, y_var, X_vars) {
  # Turn data into matrices
  y <- to_matrix(data, y_var)
  X <- to_matrix(data, X_vars)
  # Add intercept
  X <- cbind(1, X)
  # Name the new column "intercept"
  colnames(X) <- c("intercept", X_vars)
  # Calculate n and k for degrees of freedom
  n <- nrow(X)
  k <- ncol(X)
  # J is k-1
  J <- k - 1
  # Create the R matrix: bind a column of zeros
  # onto a J-by-J identity matrix
  R <- cbind(0, diag(J))

  # Estimate coefficients
  b <- b_ols(data, y_var, X_vars)
  # Retrieve OLS residuals
  e <- b$vars$e
  # Retrieve s^2
  s2 <- b$s2 %<>% as.numeric()


  # Create the inner matrix R(X'X)^(-1)R'
  RXXR <- R %*% solve(t(X) %*% X) %*% t(R)
  # Calculate the F stat
  f_stat <- t(R %*% b) %*% solve(RXXR) %*% (R %*% b) / J / s2
  # Calculate the p-value;; why normal and not chi^2
  p_value <- pf(q = f_stat, df1 = J, df2 = n-k, lower.tail = F)
  # Create a data.frame of the f stat. and p-value
  results <- data.frame(
    f_stat = f_stat %>% as.vector(),
    p_value = p_value %>% as.vector())
  return(results)
  
}

```


## Question 1:
**Read the data into R. Inspect it. Sort by size (Q (kwh)).** 


```{r read_in_data}

nerlove <- readxl::read_excel("nerlove.xls", col_names=T)
summary(nerlove)

# Fix typo in 13th row (missing a decimal!)
# DO THIS MORE ELEGANTLY! 
nerlove[13, "PL"] <- 1.81

# nerlove %>% 
#   filter(PL > 100) %>%
#     mutate(PL = PL/100)

nerlove %>% arrange(Q)

```

Plot the series and make sure your data are read in correctly. 
```{r plotseries, highlight=T}
ggplot(nerlove, aes(x=Q, y=TC)) + 
  geom_point() + 
  labs(title="Nerlove Data", x="Output (kWh)", y="Total Cost")

ggplot(nerlove, aes(x=PL, y=TC)) + 
  geom_point() + 
  labs(title="Nerlove Data", x="Price of Labor ($)", y="Total Cost")

ggplot(nerlove, aes(x=PK, y=TC)) + 
  geom_point() + 
  labs(title="Nerlove Data", x="Price of Capital ($)", y="Total Cost")

ggplot(nerlove, aes(x=PF, y=TC)) + 
  geom_point() + 
  labs(title="Nerlove Data", x="Price of Fuel ($)", y="Total Cost")

```

## Question 2:
**Replicate regression I (page 176) in the paper.**

Regression I:

$log(TC) - log(P_F) = \beta_0 + \beta_1Q + \beta_2\Bigl(log(P_L)-log(P_F)\Bigr) + \beta_3\Bigl(log(P_K)-log(P_F)\Bigr)$

Equivalent to:

$log(\frac{TC}{P_F}) = \beta_0 + \beta_1Q + \beta_2log(\frac{P_L}{P_F}) + \beta_3log(\frac{P_K}{P_F})$


Where:  
$TC$ = total production cost,

$P_L$ = wage rate,

$P_K$ = "price" of capital, 

$P_F$ = price of fuel,

$Q$ = output (measured in kWh)  



In generalized Cobb-Douglas form:

$\beta_1 = \frac{1}{r}$, 

$\beta_2 = \frac{a_L}{r}$, 

$\beta_3 = \frac{a_K}{r}$

Prepare variables for Regression I.
```{r RegressionI_prep}
# Create log variables
nerlove %<>% mutate(
  TClog = log(TC),
  Qlog = log(Q),
  PLlog = log(PL),
  PKlog = log(PK),
  PFlog = log(PF)
)

# Create PF scaled variables
nerlove %<>% mutate(
  TCscaled = TClog - PFlog,
  PLscaled = PLlog - PFlog,
  PKscaled = PKlog - PFlog
)
```

Variable names:

$log(\frac{TC}{P_F})$ = "TCscaled"

$log(\frac{P_L}{P_F})$ = "PLscaled"

$log(\frac{P_K}{P_F})$ = "PKscaled"

```{r RegressionI}

# Regression I: 
# dep var = (log costs - log fuel price) = TCscaled
reg_I <- ols(data = nerlove,y_data = "TCscaled",
             X_data = c("Qlog","PLscaled","PKscaled"),
             intercept = T, H0 = 0, alpha = 0.05)

reg_I$ttest
reg_I$mof_table

```

Coefficients are pretty close to those in the paper. $R^2$ matches! 


## Question 3:
**Conduct the hypothesis test using constant returns to scale ($\beta_1$ = 1) as your null hypothesis.**

```{r null1}

# Re-run regression with null hypothesis H0 = 1
reg_I <- ols(data = nerlove, y_data = "TCscaled",
             X_data = c("Qlog","PLscaled","PKscaled"),
             intercept = T, H0 = 1, alpha = 0.05)

reg_I$ttest
```


**What is the p- value associated with you test statistic? What is your point estimate of returns to scale? Constant? Increasing? Decreasing?**

The p-value is 0.000. The point estimate of returns to scale is $\frac{1}{\beta}=r =$ `r 1/reg_I$b[2]`, hence returns to scale is increasing.

## Question 4:
**Plot residuals against output.**

```{r plotresid}

ggplot(reg_I$vars, aes(y=e, x=Qlog)) + geom_point() + labs(title="Regression I: Residuals against Output", x="Output (log(Q))", y="Residuals")

```

**What do you notice? What does this potentially tell you from an economic perspective?**

*Evidence of heteroskedasticity: residuals seem to track the log output through parabola. We may want to rethink our specification!*



**Compute the correlation coefficient of the residuals with output for the entire sample? What does this tell you?**

```{r corr coeff}
# R = cov(xy)/var(x)var(y)
R_I <- (cov(x=reg_I$vars$e, y=reg_I$vars$Qlog)/(var(reg_I$vars$e)*var(reg_I$vars$Qlog))) %>% signif(3) %>% as.character()
R_I
```
The correlation coefficient is extremely small: `r R_I`.

## Question 5:
**Divide your sample into 5 subgroups of 29 firms each according to the level of output. Estimate the regression model again for each group separately.**

```{r split_sample}
# Divide sample into 5 subgroups
d <- split(nerlove,rep(1:5,each=29))

# Now we have a list, d, with five dataframes '1' through '5' for our five subgroups.
```

```{r Regression III}
# Regression IIIA
reg_IIIA <- ols(data = d$`1`,y_data = "TCscaled",
             X_data = c("Qlog","PLscaled","PKscaled"),
             intercept = T, H0 = 1, alpha = 0.05)
reg_IIIA$ttest

# Regression IIIB
reg_IIIB <- ols(data = d$`2`,y_data = "TCscaled",
             X_data = c("Qlog","PLscaled","PKscaled"),
             intercept = T, H0 = 1, alpha = 0.05)
reg_IIIB$ttest

# Regression IIIC
reg_IIIC <- ols(data = d$`3`,y_data = "TCscaled",
             X_data = c("Qlog","PLscaled","PKscaled"),
             intercept = T, H0 = 1, alpha = 0.05)
reg_IIIC$ttest

# Regression IIID
reg_IIID <- ols(data = d$`4`,y_data = "TCscaled",
             X_data = c("Qlog","PLscaled","PKscaled"),
             intercept = T, H0 = 1, alpha = 0.05)
reg_IIID$ttest

# Regression IIIE
reg_IIIE <- ols(data = d$`5`,y_data = "TCscaled",
             X_data = c("Qlog","PLscaled","PKscaled"),
             intercept = T, H0 = 1, alpha = 0.05)
reg_IIIE$ttest




## May want to clean this up and run as for loop???
```

**Can you replicate Equations IIIA - IIIE? Calculate the point estimates for returns to scale for each sample. Is there a pattern relating to size of output?**
Regression    | Returns to Scale
------------- | -------------
IIIA          | `r 1/ reg_IIIA$b[2]`
IIIB          | `r 1/ reg_IIIB$b[2]`
IIIC          | `r 1/ reg_IIIC$b[2]`
IIID          | `r 1/ reg_IIID$b[2]`
IIIE          | `r 1/ reg_IIIE$b[2]`

*Coefficients roughly match the results of the paper!*


## Question 6: 
**Create ”dummy variables” for each industry. Interact them with the output variable to create five ”slope coefficients”.**

``` {r createdummies}

# create group categorical var
for (i in 1:5) {
  d[[i]] %<>% mutate(gvar=i)
}

# unsplit
df <- rbind(d[[1]], d[[2]], d[[3]], d[[4]], d[[5]])

# create dummies
df %<>%
    mutate(
      g1 = ifelse(gvar==1, 1, 0),
      g2 = ifelse(gvar==2, 1, 0),
      g3 = ifelse(gvar==3, 1, 0),
      g4 = ifelse(gvar==4, 1, 0),
      g5 = ifelse(gvar==5, 1, 0))
  
df %<>% select(-gvar)

# interact with output variable
df %<>%
    mutate(
      lQ_A = Qlog*g1,
      lQ_B = Qlog*g2,
      lQ_C = Qlog*g3,
      lQ_D = Qlog*g4,
      lQ_E = Qlog*g5)

```

**Run a model, letting the intercept and slope coefficient on output differ across plants, but let the remainder of the coefficients be pooled across plants.**

```{r OLS_dummies}

# I'm not sure this is the right model... but I'm gettin similar coeffs!



reg_IV <- ols(data = df, y_data = "TCscaled",
             X_data = c("lQ_A", "lQ_B", "lQ_C", "lQ_D", "lQ_E", "PLscaled","PKscaled", "g1",
                        "g2", "g3", "g4", "g5"),
             intercept = F, H0 = 0, alpha = 0.05)

# Can also omit one of the dummie vars (g) and include an intercept >> get the same coefs
reg_IV$ttest

reg_IV$mof_table

```



**Are there any noticeable changes in returns to scale from the previous part?**

*yes....*

## Question 7:
**Conduct a statistical test comparing the first model you estimate to the last model you estimated. (Hint: Is one model a restricted version of the other?). Would separate t-test have given you the same results?**

``` {r joint_test}



```


## Question 8:
**To see whether returns to scale declined with output, Nerlove tested a nonlinear specification by including $ln(y)^2$ as a regressor. Conduct a statistical test you feel is appropriate to test this hypothesis.**


